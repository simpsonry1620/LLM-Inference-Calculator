Pulling and running NVIDIA NIM container

===========================================
== NVIDIA Inference Microservice LLM NIM ==
===========================================

NVIDIA Inference Microservice LLM NIM Version 1.3.0
Model: meta/llama-3.1-8b-instruct

Container image Copyright (c) 2016-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

The NIM container is governed by the NVIDIA Software License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement) and the Product Specific Terms for AI Products (found at https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products).

A copy of this license can be found under /opt/nim/LICENSE.

The use of this model is governed by the NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement).

ADDITIONAL INFORMATION: Llama 3.1 Community License Agreement, Built with Llama.

You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead. See https://pypi.org/project/pynvml for more information.
Profile 03a78e44c549d72b7e354b2dfbb10fc2101e007b3ea94e63b6528287f5431d40 is not fully defined with checksums
Profile 193649a2eb95e821309d6023a2cabb31489d3b690a9973c7ab5d1ff58b0aa7eb is not fully defined with checksums
Profile 1b02e4b399e8e0a5f7f49b49c4cf6b8cee33fb31854767ef386e74e8ff92762f is not fully defined with checksums
Profile 25cd80b0cf0f0989de30be57025771aea3f871d22b60c0bc088cbda0701b4b23 is not fully defined with checksums
Profile 33e38db03bb29d47b6c6e604c5e3686f224a1f88e97cd3e0e18cf83e71a949fb is not fully defined with checksums
Profile 395082aa40085d35f004dd3056d7583aea330417ed509b4315099a66cfc72bdd is not fully defined with checksums
Profile 3ad27031e57506f12b47d3cd74d045fe05977f0bb92e2f8c80a3240748482578 is not fully defined with checksums
Profile 3f67662fdffadc89d38abd02c2fcb3c756f6f874e6bddc6d8521383663abd5c4 is not fully defined with checksums
Profile 47f45a1d55aa8a03aa64f15781e2cf28cec4ff31e67b99e59f0067bf6d2f8afc is not fully defined with checksums
Profile 47fc22081f20f85bbe66f21ec4765a3c8e578e08a0254bbe8f2ddf056a64b217 is not fully defined with checksums
Profile 48696b63c4821ae61e3dae479a1a822f1d2aa4cc8d02fae64a59f1d88c487304 is not fully defined with checksums
Profile 4e0d43c3245d0232d32bcca05648c98a70e9692518701cdd0cfd987acf5a3cfa is not fully defined with checksums
Profile 6458846f873390eec5ee46d07239c4e9c976b64b84dd3585b7ff5740114a0830 is not fully defined with checksums
Profile 69545d0e42d494d0c03be120535898cf2d8e6fd9a5c0a5687a168ef6ba6501e5 is not fully defined with checksums
Profile 7cc8597690a35aba19a3636f35e7f1c7e7dbc005fe88ce9394cad4a4adeed414 is not fully defined with checksums
Profile 7fb52d5c2883c5393818bc3b0414f4a9b4b5717d6257d4b79ea227985a12bef5 is not fully defined with checksums
Profile 805f98a255898b0c7d4c68199bf8655af4ffc1a10ca0165d9aa05b9a6b0a7b8e is not fully defined with checksums
Profile 8af967d80ae8f30f4635a59b2140fdc2b38d3004e16e66c9667fa032e56497fd is not fully defined with checksums
Profile 8c27f77dab1986e76b524c755fa5a809f8882517b503e76bfcf8d42b991adc89 is not fully defined with checksums
Profile 9189d008806a9638d4206e6ff94c0b0d9acc2a8861f6de5a49b9d0a5acdcf049 is not fully defined with checksums
Profile 96e7cd0991f4ab5cf47a08cce8d1169daa8a431485be805fb00de0638bdeed9d is not fully defined with checksums
Profile 9bccc20c28c1728b59cdbad4b2c1607d3b57388ff266da4477ea8a413ae0fb7d is not fully defined with checksums
Profile b7b6fa584441d9536091ce5cf80ccc31765780b8a46540da4e7bada5c5108ed9 is not fully defined with checksums
Profile c8bfdeefde531b7176e7a37163d4ab7de1b9cede0d01f008c4bd6a87436709a2 is not fully defined with checksums
Profile d16635e67b41130625a0e0132a6205ba6ca048a19e9b0a6b61bde5087801ed08 is not fully defined with checksums
Profile df4113435195daa68b56c83741d66b422c463c556fc1669f39f923427c1c57c5 is not fully defined with checksums
Profile e4087b2f1d8c3c581d0587971960d4747884bdf9922b507253f60baad10752fd is not fully defined with checksums
Profile ebaf474b36a93eebbe8853ef58e22f95875cb45706f362d1761e7e25570d642c is not fully defined with checksums
Profile ed4af8b6563348d37f72bfd013be44573a1c88f384ef8fb3eaf0c69e4f235c20 is not fully defined with checksums
Profile f25fe2be374f7dfd85e42d3be1792f12deab691c6e2dcdf807ad4857e163b8e0 is not fully defined with checksums
Profile f8b5f71dd66c36c70deac7927cbd98b1c4f78caf1abf01f768be7118e1daa278 is not fully defined with checksums
Profile fa55c825306dfc09c9d0e7ef423e897d91fe8334a3da87d284f45f45cbd4c1b0 is not fully defined with checksums
INFO 2025-04-17 21:56:50.730 ngc_profile.py:267] Running NIM without LoRA. Only looking for compatible profiles that do not support LoRA.
INFO 2025-04-17 21:56:50.730 ngc_profile.py:269] Detected 2 compatible profile(s).
INFO 2025-04-17 21:56:50.730 ngc_injector.py:151] Valid profile: 7cc8597690a35aba19a3636f35e7f1c7e7dbc005fe88ce9394cad4a4adeed414 (tensorrt_llm-trtllm_buildable-bf16-tp1) on GPUs [0]
INFO 2025-04-17 21:56:50.730 ngc_injector.py:151] Valid profile: 193649a2eb95e821309d6023a2cabb31489d3b690a9973c7ab5d1ff58b0aa7eb (vllm-bf16-tp1) on GPUs [0]
INFO 2025-04-17 21:56:50.730 ngc_injector.py:271] Selected profile: 7cc8597690a35aba19a3636f35e7f1c7e7dbc005fe88ce9394cad4a4adeed414 (tensorrt_llm-trtllm_buildable-bf16-tp1)
INFO 2025-04-17 21:56:50.731 ngc_injector.py:279] Profile metadata: feat_lora: false
INFO 2025-04-17 21:56:50.731 ngc_injector.py:279] Profile metadata: llm_engine: tensorrt_llm
INFO 2025-04-17 21:56:50.731 ngc_injector.py:279] Profile metadata: precision: bf16
INFO 2025-04-17 21:56:50.731 ngc_injector.py:279] Profile metadata: tp: 1
INFO 2025-04-17 21:56:50.731 ngc_injector.py:279] Profile metadata: trtllm_buildable: true
INFO 2025-04-17 21:56:50.737 launch.py:46] engine_world_size=1
INFO 2025-04-17 21:56:50.738 launch.py:92] running command ['/opt/nim/llm/.venv/bin/python3', '-m', 'nim_llm_sdk.entrypoints.openai.api_server', '--served-model-name', 'meta/llama-3.1-8b-instruct', '--async-engine-args', '{"model": "/tmp/LLM-v6j7pgq3", "served_model_name": ["meta/llama-3.1-8b-instruct"], "tokenizer": "/tmp/LLM-v6j7pgq3", "skip_tokenizer_init": false, "tokenizer_mode": "auto", "trust_remote_code": false, "download_dir": null, "load_format": "auto", "dtype": "bfloat16", "kv_cache_dtype": "auto", "quantization_param_path": null, "seed": 0, "max_model_len": null, "worker_use_ray": false, "distributed_executor_backend": "mp", "pipeline_parallel_size": 1, "tensor_parallel_size": 1, "max_parallel_loading_workers": null, "block_size": 16, "enable_prefix_caching": false, "disable_sliding_window": false, "use_v2_block_manager": false, "swap_space": 4, "cpu_offload_gb": 0, "gpu_memory_utilization": 0.9, "max_num_batched_tokens": null, "max_num_seqs": 256, "max_logprobs": 20, "disable_log_stats": false, "revision": null, "code_revision": null, "rope_scaling": null, "rope_theta": null, "tokenizer_revision": null, "quantization": null, "enforce_eager": false, "max_context_len_to_capture": null, "max_seq_len_to_capture": 8192, "disable_custom_all_reduce": false, "tokenizer_pool_size": 0, "tokenizer_pool_type": "ray", "tokenizer_pool_extra_config": null, "limit_mm_per_prompt": null, "enable_lora": false, "enable_dora": false, "max_loras": 8, "max_lora_rank": 32, "enable_prompt_adapter": false, "max_prompt_adapters": 1, "max_prompt_adapter_token": 0, "fully_sharded_loras": false, "lora_extra_vocab_size": 256, "long_lora_scaling_factors": null, "lora_dtype": "auto", "max_cpu_loras": 16, "peft_source": null, "peft_refresh_interval": null, "device": "auto", "num_scheduler_steps": 1, "ray_workers_use_nsight": false, "num_gpu_blocks_override": null, "num_lookahead_slots": 0, "model_loader_extra_config": null, "ignore_patterns": [], "preemption_mode": null, "scheduler_delay_factor": 0.0, "enable_chunked_prefill": null, "guided_decoding_backend": "outlines", "speculative_model": null, "speculative_model_quantization": null, "speculative_draft_tensor_parallel_size": null, "num_speculative_tokens": null, "speculative_max_model_len": null, "speculative_disable_by_batch_size": null, "ngram_prompt_lookup_max": null, "ngram_prompt_lookup_min": null, "spec_decoding_acceptance_method": "rejection_sampler", "typical_acceptance_sampler_posterior_threshold": null, "typical_acceptance_sampler_posterior_alpha": null, "qlora_adapter_name_or_path": null, "disable_logprobs_during_spec_decoding": null, "otlp_traces_endpoint": null, "collect_detailed_traces": null, "engine_use_ray": false, "disable_log_requests": true, "selected_gpus": [{"name": "NVIDIA RTX 6000 Ada Generation", "device_index": 0, "device_id": "26b1:10de", "total_memory": 51527024640, "free_memory": 50769166336, "used_memory": 124387328, "reserved_memory": 633470976, "compute_capability": [8, 9], "family": "RTX A6000 Ada"}], "tllm_buildable": true, "tllm_config_json_str": null, "profile_id": "7cc8597690a35aba19a3636f35e7f1c7e7dbc005fe88ce9394cad4a4adeed414", "quantization_algo": null}']
[1744927014.558380] [122274cf5d1e:55   :0]          parser.c:2305 UCX  WARN  unused environment variables: UCX_HOME; UCX_DIR (maybe: UCX_TLS?)
[1744927014.558380] [122274cf5d1e:55   :0]          parser.c:2305 UCX  WARN  (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead. See https://pypi.org/project/pynvml for more information.
2025-04-17 21:57:06,119 [WARNING] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: error
2025-04-17 21:57:06,119 [INFO] [TRT-LLM] [I] Starting TensorRT-LLM init.
2025-04-17 21:57:06,128 [INFO] [TRT-LLM] [I] TensorRT-LLM inited.
[TensorRT-LLM] TensorRT-LLM version: 0.12.3
INFO 2025-04-17 21:57:06.428 api_server.py:722] NIM LLM API version 1.3.0
INFO 2025-04-17 21:57:06.431 utils.py:237] Using provided selected GPUs list [0]
INFO 2025-04-17 21:57:06.465 dynamic_module_loader.py:92] Loading dynamic modules from /opt/nim/llm/nim_llm_sdk/model_specific_modules
WARNING 2025-04-17 21:57:06.472 arg_utils.py:857] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 2025-04-17 21:57:06.472 config.py:916] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 2025-04-17 21:57:06.472 async_trtllm_engine.py:81] Initializing an LLM engine (v1.3.0) with config: model='/tmp/LLM-v6j7pgq3', speculative_config=None, tokenizer='/tmp/LLM-v6j7pgq3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 2025-04-17 21:57:06.782 utils.py:285] Building rank 0 with config ConstrainedTrtllmConfig(dtype='bfloat16', pp_size=1, tp_size=1, max_batch_size=512, max_seq_len=8192, max_num_tokens=8192, lora=ConstrainedTrtllmLoraConfig(enable=False, modules=['attn_qkv', 'attn_q', 'attn_k', 'attn_v', 'attn_dense', 'mlp_h_to_4h', 'mlp_4h_to_h', 'mlp_gate'], max_rank=64), quant_config=None, gpus_per_node=1)
INFO 2025-04-17 21:57:06.782 build_utils.py:144] Detected model architecture: LlamaForCausalLM
230it [00:00, 1411.57it/s]
INFO 2025-04-17 21:57:07.116 build_utils.py:192] Total time of reading and converting rank0 checkpoint: 0.33 s
INFO 2025-04-17 21:57:07.116 logger.py:93] [TRT-LLM] [I] Set dtype to bfloat16.
WARNING 2025-04-17 21:57:07.117 logger.py:93] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width.

WARNING 2025-04-17 21:57:07.117 logger.py:93] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored
INFO 2025-04-17 21:57:09.942 logger.py:93] [TRT-LLM] [I] Set nccl_plugin to None.
INFO 2025-04-17 21:57:10.320 logger.py:93] [TRT-LLM] [I] Total optimization profiles added: 1
INFO 2025-04-17 21:57:10.322 logger.py:93] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0
INFO 2025-04-17 21:58:57.162 logger.py:93] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:01:46
INFO 2025-04-17 21:58:57.163 logger.py:93] [TRT-LLM] [I] Build phase peak memory: 41422.48 MB, children: 15.50 MB
INFO 2025-04-17 21:58:57.163 build_utils.py:204] Total time of building rank0 engine: 110.05 s
WARNING 2025-04-17 21:58:57.343 logger.py:93] [TRT-LLM] [W] Implicitly setting PretrainedConfig.mlp_bias = False
WARNING 2025-04-17 21:58:57.343 logger.py:93] [TRT-LLM] [W] Implicitly setting PretrainedConfig.attn_bias = False
WARNING 2025-04-17 21:58:57.343 logger.py:93] [TRT-LLM] [W] Implicitly setting PretrainedConfig.rotary_base = 500000.0
WARNING 2025-04-17 21:58:57.343 logger.py:93] [TRT-LLM] [W] Implicitly setting PretrainedConfig.rotary_scaling = {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
WARNING 2025-04-17 21:58:57.343 logger.py:93] [TRT-LLM] [W] Implicitly setting PretrainedConfig.residual_mlp = False
WARNING 2025-04-17 21:58:57.343 logger.py:93] [TRT-LLM] [W] Implicitly setting PretrainedConfig.disable_weight_only_quant_plugin = False
WARNING 2025-04-17 21:58:57.343 logger.py:93] [TRT-LLM] [W] Implicitly setting PretrainedConfig.moe = {'num_experts': 0, 'top_k': 0, 'normalization_mode': None, 'tp_mode': 0}
WARNING 2025-04-17 21:58:57.343 logger.py:93] [TRT-LLM] [W] Implicitly setting PretrainedConfig.remove_duplicated_kv_heads = False
INFO 2025-04-17 21:58:58.206 utils.py:237] Using provided selected GPUs list [0]
INFO 2025-04-17 21:58:58.214 utils.py:354] Using 0 bytes of gpu memory for PEFT cache
INFO 2025-04-17 21:58:58.214 utils.py:359] Engine size in bytes 16149896228
INFO 2025-04-17 21:58:58.215 utils.py:363] available device memory 50233016320
INFO 2025-04-17 21:58:58.215 utils.py:374] Setting free_gpu_memory_fraction to 0.9
INFO 2025-04-17 21:59:19.158 serving_chat.py:96] Using supplied tool use configs
INFO 2025-04-17 21:59:19.158 serving_chat.py:96] Using supplied tool use configs